{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0s4CGu7hK6nQvyfefWByB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yug-sinha/ANSWER-RETRIEVAL-EVALUATOR-SYSTEM/blob/main/ClearFeed(ml_ans).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn google-generativeai rouge-score bert-score"
      ],
      "metadata": {
        "id": "Nf8GyaDsfO30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0elJ_-zx5o32"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import google.generativeai as genai\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths (hardcoded)\n",
        "KB_FILE = '/Clearfeed_kb.json'\n",
        "EVAL_FILE = '/clearfeed_qa_pairs.csv'"
      ],
      "metadata": {
        "id": "JLMYrNna5xvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Path According to your Requirements after Uploading the files"
      ],
      "metadata": {
        "id": "Jc4lVHwvsi3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the JSON dataset\n",
        "with open(KB_FILE, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Flatten JSON into a DataFrame\n",
        "urls, titles, texts = [], [], []\n",
        "\n",
        "for url, content in data.items():\n",
        "    urls.append(url)\n",
        "    titles.append(content['title'])\n",
        "    texts.append(content['text'])\n",
        "\n",
        "corpus_df = pd.DataFrame({'url': urls, 'title': titles, 'text': texts})\n",
        "\n",
        "# Combine title and text for TF-IDF vectorization\n",
        "corpus_df['content'] = corpus_df['title'] + \" \" + corpus_df['text']"
      ],
      "metadata": {
        "id": "wo-R63r_578n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### **Explanation of the Approach: TF-IDF Vectorization with Cosine Similarity**\n",
        "\n",
        "1. **TF-IDF Vectorization**:\n",
        "   - **What It Does**:\n",
        "     TF-IDF (Term Frequency-Inverse Document Frequency) represents text data as numerical vectors, emphasizing the importance of words in a document relative to the entire corpus.\n",
        "     - **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n",
        "     - **Inverse Document Frequency (IDF)**: Reduces the weight of common terms across all documents, giving higher importance to unique terms.\n",
        "   - **Purpose**: Converts textual content into numerical form suitable for similarity calculations.\n",
        "\n",
        "2. **Cosine Similarity**:\n",
        "   - **What It Does**:\n",
        "     Measures the cosine of the angle between two vectors (query vector and document vectors). Values range from 0 (completely dissimilar) to 1 (identical).\n",
        "   - **Purpose**: Determines how similar the query is to each document in the corpus.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It's Used in the Code**:\n",
        "\n",
        "- **Step 1**: `TfidfVectorizer` is initialized and fit on the `corpus_df['content']`, creating a matrix (`tfidf_matrix`) where each row represents a document's vector.\n",
        "- **Step 2**: When a question is input by the user, it is transformed into a query vector using the same TF-IDF vectorizer.\n",
        "- **Step 3**: Cosine similarity is computed between the query vector and all document vectors in the `tfidf_matrix`.\n",
        "- **Step 4**: The documents are ranked based on similarity scores, and the top `k` URLs (default is 5) are retrieved and displayed.\n",
        "\n",
        "This approach efficiently retrieves the most relevant URLs based on term importance and contextual overlap."
      ],
      "metadata": {
        "id": "3YTHUOfeNCfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Build a TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_df['content'])\n",
        "\n",
        "def retrieve_top_k_urls(question, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieve the top 5 URLs for a given question.\n",
        "    \"\"\"\n",
        "    query_vec = tfidf_vectorizer.transform([question])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix)\n",
        "    top_indices = scores[0].argsort()[-top_k:][::-1]\n",
        "    return corpus_df.iloc[top_indices][['url', 'title']].reset_index(drop=True)\n",
        "\n",
        "# 1. Take a question input from the user\n",
        "question = input(\"Enter your question: \")\n",
        "top_results = retrieve_top_k_urls(question)\n",
        "\n",
        "# Format and print the top URLs\n",
        "print(\"\\nTop 5 URLs:\")\n",
        "for idx, row in top_results.iterrows():\n",
        "    print(f\"{idx + 1}) {row['url']}\")"
      ],
      "metadata": {
        "id": "uFIReWVV5-eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the Google Generative AI SDK with the API key\n",
        "GEMINI_API_KEY = \"AIzaSyCrrLhhFIDWW3AGA8TZvLAVURvzVm4Ry30\" #Committed API Key\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Load the JSON knowledge base\n",
        "with open('/Clearfeed_kb.json', 'r') as f: #Update Path According to your requirements\n",
        "    knowledge_base = json.load(f)\n",
        "\n",
        "def generate_answer_from_gemini(question, top_results):\n",
        "    \"\"\"\n",
        "    Generate an answer using Google Gemini API and the top 5 results from the JSON knowledge base.\n",
        "    \"\"\"\n",
        "    # Prepare the context from the top 5 results\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"{row['title']}:\\n{knowledge_base[row['url']]['text']}\" for _, row in top_results.iterrows()\n",
        "    )\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"Using the following knowledge base, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    # Configure and start a chat session\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-1.5-flash\",  # Use the correct model name\n",
        "        generation_config={\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"max_output_tokens\": 150,\n",
        "            \"response_mime_type\": \"text/plain\"\n",
        "        }\n",
        "    )\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "\n",
        "    # Return the response text\n",
        "    return response.text if response else \"No response from Gemini API\"\n",
        "\n",
        "# Generate the answer using only the top 5 results\n",
        "generated_answer = generate_answer_from_gemini(question, top_results)\n",
        "print(\"Generated Answer:\")\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "InVVgL3S6D_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(question):\n",
        "    \"\"\"\n",
        "    Evaluate the model's answer generation quality using ROUGE-L scores.\n",
        "    \"\"\"\n",
        "        # Initialize the ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)  # Using ROUGE-L for evaluation\n",
        "\n",
        "    # Combine the content of the top results\n",
        "    context = \"\\n\\n\".join(\n",
        "    f\"{row['title']}:\\n{knowledge_base[row['url']]['text']}\" for _, row in top_results.iterrows()\n",
        ")\n",
        "\n",
        "    # Calculate ROUGE-L score between the generated answer and the combined content\n",
        "    score = scorer.score(context, generated_answer)\n",
        "\n",
        "    return score['rougeL'].fmeasure  # ROUGE-L F1 score\n",
        "\n",
        "# Evaluate the system and print the results\n",
        "print(\"\\nEvaluating the model's performance...\")\n",
        "rougeL_score = evaluate_model(question)\n",
        "print(\"\\nROUGE-L Score:\", rougeL_score)"
      ],
      "metadata": {
        "id": "6xWaaaV46PNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Approach: ROUGE-L Evaluation**\n",
        "\n",
        "1. **ROUGE-L Metric**:\n",
        "   - **What It Does**: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap of the longest common subsequence (LCS) between the generated text and reference text.  \n",
        "     - **Precision**: How much of the generated text aligns with the reference.  \n",
        "     - **Recall**: How much of the reference text aligns with the generated text.  \n",
        "     - **F1-Score**: A harmonic mean of precision and recall.  \n",
        "   - **Purpose**: Evaluates the fluency and structural similarity between generated and reference content, focusing on sentence-level coherence.\n",
        "\n",
        "2. **Advantages**:\n",
        "   - Captures both order and sequence of words.  \n",
        "   - Suitable for comparing long text summaries.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It's Used in the Code**:\n",
        "\n",
        "- **Step 1**: The `RougeScorer` is initialized with `rougeL` and configured to use stemming to normalize terms.  \n",
        "- **Step 2**: The context is prepared by concatenating the titles and corresponding text of the top retrieved results.  \n",
        "- **Step 3**: The `generated_answer` is compared with the combined `context` using ROUGE-L, calculating the LCS overlap.  \n",
        "- **Step 4**: The `F1-Score` from ROUGE-L is returned as the evaluation metric, providing a balanced measure of how well the generated answer matches the context.  \n",
        "\n",
        "This implementation uses ROUGE-L to assess the quality of generated answers by comparing their structural and linguistic alignment with the input context."
      ],
      "metadata": {
        "id": "2GhhKiPOPxQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_with_factual_consistency(question):\n",
        "    \"\"\"\n",
        "    Evaluate the model's answer generation quality using BERTScore for factual consistency.\n",
        "    \"\"\"\n",
        "    # Combine the content of the top results\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"{row['title']}:\\n{knowledge_base[row['url']]['text']}\" for _, row in top_results.iterrows()\n",
        "    )\n",
        "\n",
        "    # Calculate BERTScore between the generated answer and the combined content\n",
        "    P, R, F1 = bert_score([generated_answer], [context], lang=\"en\")  # Ensure the language matches\n",
        "    return F1.mean().item()  # Return the mean F1 score as a factual consistency metric\n",
        "\n",
        "# Evaluate the system and print the results\n",
        "print(\"\\nEvaluating the model's performance with factual consistency...\")\n",
        "factual_consistency_score = evaluate_model_with_factual_consistency(question)\n",
        "print(\"\\nFactual Consistency (BERTScore F1):\", factual_consistency_score)"
      ],
      "metadata": {
        "id": "9qzrEYMpeqzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Approach: BERTScore Evaluation**\n",
        "\n",
        "1. **BERTScore Metric**:\n",
        "   - **What It Does**: BERTScore evaluates the semantic similarity between two pieces of text by leveraging pre-trained BERT embeddings. It compares word-level representations of the generated text and the reference (or context) using precision, recall, and F1 scores.\n",
        "     - **Precision**: Measures how well the generated answer’s words match the reference.\n",
        "     - **Recall**: Measures how much of the reference's words are captured by the generated answer.\n",
        "     - **F1-Score**: A balance between precision and recall, indicating overall similarity.\n",
        "   - **Purpose**: Unlike traditional metrics like ROUGE, which focus on exact word overlap, BERTScore measures semantic consistency, making it suitable for evaluating the factual correctness and fluency of generated content.\n",
        "\n",
        "2. **Advantages**:\n",
        "   - Captures deeper semantic meaning by using contextual embeddings from BERT.\n",
        "   - Handles synonymy and paraphrasing better than surface-level overlap metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It's Used in the Code**:\n",
        "\n",
        "- **Step 1**: The context is prepared by concatenating the titles and text of the top relevant URLs from the knowledge base.  \n",
        "- **Step 2**: The `generated_answer` is compared with the context using BERTScore. It calculates the precision, recall, and F1 score for the semantic similarity between the two texts.\n",
        "- **Step 3**: The mean F1 score is returned, which is used as the factual consistency metric to evaluate how well the generated answer aligns with the factual content in the context.\n",
        "\n",
        "This approach uses BERTScore to assess how factually consistent and semantically aligned the model's generated answer is with the provided content, ensuring that the response is both relevant and accurate."
      ],
      "metadata": {
        "id": "L5OrErG1PsOC"
      }
    }
  ]
}